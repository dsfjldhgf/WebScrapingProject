{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a9099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "\n",
    "# Use webdriver to open Chrome\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://newtype.us')\n",
    "\n",
    "\n",
    "# Wait for the driver to fully scrape all the data\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# Initializing multiple lists\n",
    "urls, name_list, manufa_list, \n",
    "price_list, cate_list, scale_list, \n",
    "brand_list, series_list, series_list2, \n",
    "review_list, whether_review_list, \n",
    "avail_list, cate_list = ([] for i in range(13))\n",
    "\n",
    "\n",
    "# Retrieving all url links from the drop down menu\n",
    "drop_down = driver.find_elements(By.CSS_SELECTOR, \"a[class='text-strong dark:text-strong-dark hover:underline text-lg overflow-ellipsis lg:text-xs']\")\n",
    "drop_down_link_list = [link.get_attribute('href') for link in drop_down]\n",
    "\n",
    "\n",
    "# Remove links from the drop_down_link_list that are duplicate and unuseful\n",
    "for link in drop_down_link_list:\n",
    "    if link not in [\n",
    "        'https://newtype.us/t/modelkit/brand/gundam', 'https://newtype.us/v/kotobukiya/modelkit', \n",
    "        'https://newtype.us/t/modelkit/brand/mechatro', 'https://newtype.us/v/hasegawa/modelkit', \n",
    "        'https://newtype.us/t/airbrush/airbrushaccessory',\n",
    "        'https://newtype.us/v/sparmax',\n",
    "        'https://newtype.us/v/modelbingo/modelkit',\n",
    "        'https://newtype.us/v/bandai/modelkit', \n",
    "        'https://newtype.us/t/tool/cleaning', 'https://newtype.us/t/tool/cutting', \n",
    "        'https://newtype.us/t/tool/cuttingmat', 'https://newtype.us/t/tool/electric', \n",
    "        'https://newtype.us/t/tool/file', 'https://newtype.us/t/tool/glue', \n",
    "        'https://newtype.us/t/tool/knife', 'https://newtype.us/t/tool/masking', \n",
    "        'https://newtype.us/t/tool/measuring', 'https://newtype.us/t/tool/nipper', \n",
    "        'https://newtype.us/t/tool/organization', 'https://newtype.us/t/tool/paintbrush', \n",
    "        'https://newtype.us/t/tool/painting', 'https://newtype.us/t/tool/plier', \n",
    "        'https://newtype.us/t/tool/polishing', 'https://newtype.us/t/tool/putty', \n",
    "        'https://newtype.us/t/tool/sanding', 'https://newtype.us/t/tool/separator', \n",
    "        'https://newtype.us/t/tool/scribing', 'https://newtype.us/t/tool/swab', \n",
    "        'https://newtype.us/t/tool/tweezer', 'https://newtype.us/t/tool/toolset', \n",
    "        'https://newtype.us/t/tool/vise', 'https://newtype.us/t/airbrush/airbrush', \n",
    "        'https://newtype.us/t/airbrush/compressor', 'https://newtype.us/t/airbrush/airbrushcleaner', \n",
    "        'https://newtype.us/t/airbrush/airbrushaccessory', 'https://newtype.us/t/airbrush/airbrushpart', \n",
    "        'https://newtype.us/v/newtype/giftcard','https://newtype.us/v/samawangu',\n",
    "        'https://newtype.us/v/newtype/accessory/decal', \n",
    "        'https://newtype.us/v/newtype/giftcard', \n",
    "        'https://newtype.us/v/newtype/sticker'\n",
    "        'https://newtype.us/v/mcms', 'https://newtype.us/v/nostar', \n",
    "        'https://newtype.us/t/accessory/actionbase', \n",
    "        'https://newtype.us/t/accessory/decal', \n",
    "        'https://newtype.us/t/accessory/diorama', \n",
    "        'https://newtype.us/t/accessory/effect', \n",
    "        'https://newtype.us/t/accessory/optionpart', \n",
    "        'https://newtype.us/t/accessory/photoetch', \n",
    "        'https://newtype.us/t/accessory/led', \n",
    "        'https://newtype.us/t/material/aluminum', \n",
    "        'https://newtype.us/t/material/brass', \n",
    "        'https://newtype.us/t/material/plastic', \n",
    "        'https://newtype.us/t/material/spring']:\n",
    "        urls.append(link)\n",
    "\n",
    "# Open each link in the selected drop down list \n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "    \n",
    "    # If a page can be scrolled down, we would do the following\n",
    "    \n",
    "    # Reference: https://stackoverflow.com/questions/20986631/how-can-i-scroll-a-web-page-using-selenium-webdriver-in-python\n",
    "    \n",
    "    time.sleep(3)\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        time.sleep(3)\n",
    "        new_height = driver.execute_script('return document.body.scrollHeight')        \n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    " \n",
    "    # Scrape prices from each page\n",
    "    prices=driver.find_elements(By.CSS_SELECTOR, \"#main > div > div > div.md\\:grid-cols-1-3.lg\\:grid-cols-1-4.xl\\:grid-cols-1-5.grid.gap-2.grid-cols-1.ais-InstantSearch.ais-InstantSearch--ssr > div.col-start-1.row-start-2.md\\:col-start-2.ais-InfiniteHits > div.grid.gap-2.grid-cols-2.md\\:grid-cols-3.lg\\:grid-cols-4.xl\\:grid-cols-5 > div > div > div.flex.col-start-1.row-start-4.items-center.text-sm > span\")\n",
    "    for price in prices:\n",
    "        price_list.append(price.text)\n",
    "\n",
    "    \n",
    "    # Scrape extra links from each page and store them \n",
    "    links = driver.find_elements(By.CSS_SELECTOR, \"a[class = 'text-strong dark:text-strong-dark col-end-3 col-start-1 row-start-2 text-base uppercase']\")\n",
    "    link_list = [link.get_attribute('href') for link in links]\n",
    "    \n",
    "    \n",
    "\n",
    "    # Open each extra link from the list that we just created\n",
    "    for url in link_list:\n",
    "        driver.get(url) \n",
    "        \n",
    "        # Scrape product name from each extra link\n",
    "        names=driver.find_elements(By.CSS_SELECTOR, \"#main > div > div.lg\\:row-end-10.contents.lg\\:block.lg\\:col-start-2.lg\\:row-start-1 > div > div.grid.grid-cols-2.pt-3.px-3.text-sm.md\\:pr-0.lg\\:px-0.col-start-1.row-start-1 > h2\")\n",
    "        for name in names:\n",
    "            name_list.append(name.text)\n",
    "            \n",
    "        # Scrape manufacturer from each extra link\n",
    "        manufa=driver.find_elements(By.CSS_SELECTOR, \"#main > div > div.lg\\:row-end-10.contents.lg\\:block.lg\\:col-start-2.lg\\:row-start-1 > div > div.grid.grid-cols-2.pt-3.px-3.text-sm.md\\:pr-0.lg\\:px-0.col-start-1.row-start-1 > a.col-start-1.row-start-1\")\n",
    "        for detail in manufa:\n",
    "            manufa_list.append(detail.text)\n",
    "            \n",
    "        # Scrape category from each extra link\n",
    "        cate=driver.find_elements(By.CSS_SELECTOR, \"#main > div > div.lg\\:row-end-10.contents.lg\\:block.lg\\:col-start-2.lg\\:row-start-1 > div > div.grid.grid-cols-2.pt-3.px-3.text-sm.md\\:pr-0.lg\\:px-0.col-start-1.row-start-1 > a.col-start-1.row-start-3.whitespace-nowrap\")\n",
    "        for detail in cate:\n",
    "            cate_list.append(detail.text)\n",
    "\n",
    "        \n",
    "        # Scrape scale from each extra link, if no scale exists, leave None in the list\n",
    "        if driver.find_elements(By.CSS_SELECTOR, \"#main > div > div.lg\\:row-end-10.contents.lg\\:block.lg\\:col-start-2.lg\\:row-start-1 > div > div.grid.gap-3.grid-cols-1.p-3.text-sm.md\\:pr-0.lg\\:pl-0.col-start-1.row-start-4 > table > tbody > tr:nth-child(1) > td:nth-child(2) \"):\n",
    "            scale=driver.find_elements(By.CSS_SELECTOR, \"#main > div > div.lg\\:row-end-10.contents.lg\\:block.lg\\:col-start-2.lg\\:row-start-1 > div > div.grid.gap-3.grid-cols-1.p-3.text-sm.md\\:pr-0.lg\\:pl-0.col-start-1.row-start-4 > table > tbody > tr:nth-child(1) > td:nth-child(2) \" )\n",
    "            for detail in scale:\n",
    "                scale_list.append(detail.text)\n",
    "        else:\n",
    "            scale_list.append('None')\n",
    "\n",
    "\n",
    "        # Scrape brand from each extra link, if no brand exists, leave None in the list\n",
    "        if driver.find_elements(By.CSS_SELECTOR, '#main > div > div.lg\\:row-end-10.contents.lg\\:block.lg\\:col-start-2.lg\\:row-start-1 > div > div.grid.gap-3.grid-cols-1.p-3.text-sm.md\\:pr-0.lg\\:pl-0.col-start-1.row-start-4 > table > tbody > tr:nth-child(2) > td:nth-child(2) > a' ):\n",
    "            brand=driver.find_elements(By.CSS_SELECTOR, '#main > div > div.lg\\:row-end-10.contents.lg\\:block.lg\\:col-start-2.lg\\:row-start-1 > div > div.grid.gap-3.grid-cols-1.p-3.text-sm.md\\:pr-0.lg\\:pl-0.col-start-1.row-start-4 > table > tbody > tr:nth-child(2) > td:nth-child(2) > a')\n",
    "            for detail in brand:\n",
    "                brand_list.append(detail.text)\n",
    "        else:\n",
    "            brand_list.append('None')\n",
    "\n",
    "\n",
    "        if driver.find_elements(By.CSS_SELECTOR, '#main > div > div.lg\\:row-end-10.contents.lg\\:block.lg\\:col-start-2.lg\\:row-start-1 > div > div.grid.gap-3.grid-cols-1.p-3.text-sm.md\\:pr-0.lg\\:pl-0.col-start-1.row-start-4 > table > tbody > tr:nth-child(3) > td:nth-child(2) > a' ):\n",
    "            series=driver.find_elements(By.CSS_SELECTOR, '#main > div > div.lg\\:row-end-10.contents.lg\\:block.lg\\:col-start-2.lg\\:row-start-1 > div > div.grid.gap-3.grid-cols-1.p-3.text-sm.md\\:pr-0.lg\\:pl-0.col-start-1.row-start-4 > table > tbody > tr:nth-child(3) > td:nth-child(2) > a')\n",
    "            for detail in series:\n",
    "                series_list.append(detail.text)\n",
    "        else:\n",
    "            series_list.append('None')\n",
    "            \n",
    "\n",
    "        if driver.find_elements(By.CSS_SELECTOR, '#main > div > div.lg\\:row-end-10.contents.lg\\:block.lg\\:col-start-2.lg\\:row-start-1 > div > div.grid.gap-3.grid-cols-1.p-3.text-sm.md\\:pr-0.lg\\:pl-0.col-start-1.row-start-4 > table > tbody > tr:nth-child(4) > td:nth-child(2) > a' ):\n",
    "            series2=driver.find_elements(By.CSS_SELECTOR, '#main > div > div.lg\\:row-end-10.contents.lg\\:block.lg\\:col-start-2.lg\\:row-start-1 > div > div.grid.gap-3.grid-cols-1.p-3.text-sm.md\\:pr-0.lg\\:pl-0.col-start-1.row-start-4 > table > tbody > tr:nth-child(4) > td:nth-child(2) > a')\n",
    "            for detail in series2:\n",
    "                series_list2.append(detail.text)\n",
    "        else:\n",
    "            series_list2.append('None')\n",
    "            \n",
    "            \n",
    "        # Scrape review from each extra link, if 'No reviews' exists, leave as what it is. Otherwise, scrape the rating\n",
    "        whether_review=driver.find_elements(By.CSS_SELECTOR, \"a[class='underline whitespace-nowrap']\")\n",
    "        for detail in whether_review:\n",
    "            if [detail.text][0] == 'No reviews':\n",
    "                review_list.append(detail.text)\n",
    "            else:\n",
    "                review_rating=driver.find_elements(By.CSS_SELECTOR, \"div[class='text-review flex col-start-1 row-end-3 row-start-1 self-center justify-end text-4xl font-medium']\")\n",
    "                for detail in review_rating:\n",
    "                    review_list.append(detail.text)\n",
    "\n",
    "        # Scrape stock availability from each extra link\n",
    "        if driver.find_elements(By.CSS_SELECTOR, \"div[class = 'stock-tag bg-green text-white']\"):\n",
    "            avail=driver.find_elements(By.CSS_SELECTOR, \"div[class = 'stock-tag bg-green text-white']\")\n",
    "            for detail in avail:\n",
    "                avail_list.append(detail.text)\n",
    "        elif driver.find_elements(By.CSS_SELECTOR, \"div[class = 'stock-tag bg-yellow text-black']\"):\n",
    "            avail_list.append('In stock')\n",
    "        elif driver.find_elements(By.CSS_SELECTOR, \"#main > div > div.lg\\:row-end-10.contents.lg\\:block.lg\\:col-start-2.lg\\:row-start-1 > div > div.grid.grid-cols-2.pt-3.px-3.text-sm.md\\:pr-0.lg\\:px-0.col-start-1.row-start-1 > div.col-start-2.row-start-4.self-center.justify-self-end > div > span.md\\:hidden\") or driver.find_elements(By.CSS_SELECTOR, \"#main > div > div.lg\\:row-end-10.contents.lg\\:block.lg\\:col-start-2.lg\\:row-start-1 > div > div.grid.grid-cols-2.pt-3.px-3.text-sm.md\\:pr-0.lg\\:px-0.col-start-1.row-start-1 > div.col-start-2.row-start-4.self-center.justify-self-end > div > span.md\\:hidden\"):\n",
    "            avail_list.append('Coming Soon')\n",
    "        else:\n",
    "            avail3=driver.find_elements(By.CSS_SELECTOR, \"div[class = 'stock-tag bg-red text-white']\")\n",
    "            for detail in avail3:\n",
    "                avail_list.append(detail.text)\n",
    "\n",
    "\n",
    "# Put all the data together by using Pandas Dataframe\n",
    "name_pd_series = pd.Series(name_list, dtype='object', name = 'Name')\n",
    "price_pd_series = pd.Series(price_list, dtype='object', name = 'Price')\n",
    "stock_pd_series = pd.Series(manufa_list, dtype='object', name = 'Manufacturer')\n",
    "cate_pd_series = pd.Series(cate_list, dtype='object', name = 'Category')\n",
    "avail_pd_series = pd.Series(avail_list, dtype='object', name = 'Stock Availability')\n",
    "scale_pd_series = pd.Series(scale_list, dtype='object', name = 'Scale')\n",
    "brand_pd_series = pd.Series(brand_list, dtype='object', name = 'Type')\n",
    "series_pd_series = pd.Series(series_list, dtype='object', name = 'Brand')\n",
    "series2_pd_series = pd.Series(series_list2, dtype='object', name = 'Series')\n",
    "review_pd_series = pd.Series(review_list, dtype='object', name = 'Review')\n",
    "dataframe = pd.concat([name_pd_series, price_pd_series, stock_pd_series, cate_pd_series, avail_pd_series, scale_pd_series, brand_pd_series, series_pd_series, series2_pd_series, review_pd_series], axis=1)\n",
    "\n",
    "\n",
    "# Save the file\n",
    "filename='Web Scrapping Project.csv'\n",
    "dataframe.to_csv(filename)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
